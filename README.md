# Text_Independent_Speaker_Recognition
I used Reynolds, D. and Rose, R. ‘Robust text-independent speaker identification using Gaussian mixture speaker models’, IEEE Trans. Speech Audio Processing, vol.3, no.1, pp. 72-83, 1995 paper to implement the project. 
In this project, I implemented one of the state-of-art algorithm, Text-Independent Speaker Recognition using MFCC and GMM proposed by D. Reynolds in 1995.No two individuals sound same because of their vocal tract structure, larynx size and other parts of their voice production organs are different. In addition to these physical differences, each speaker has his own characteristics way of speaking including the use of particular ascent, rhythm, intonation style, pronunciation pattern, choice of vocabulary and so on. From the spoken utterance, we need to extract the speaker dependent features. It is proven that the spectral features of an utterance closely represent the vocal tract structure of a person. As different persons have different vocal tract structure, their spectral features differ from one another.
Firstly we need to extract the speaker dependent feature from the speech and this can be done using Mel Frequency Cepstral Coefficients (MFCC). Secondly, we need to train a Gaussian Mixture Model on the extracted features. Thirdly we need to identify the speaker given his speech.
I have chosen TIMIT dataset to train and test my model. In this project, I am trying to examine how the order of model, choice of covariance (nodal- diagonal, full and grand-diagonal), initialization methods (k-means, random) effects the accuracy of recognizing the speaker. In this project I am using TIMIT dataset from which I used only 86 speakers. Every speaker has 10 audio clips in .wav format and I used 8 audio clips for training my model and remaining 2 audio clips to test my model. The training speaker audio files are under Train Folder and testing speaker audio files are under Test Folder. The results are stored in "result" folder.
In my reference paper for this project, 96.8% accuracy is achieved using GMM-4 at order 16. We can try to improve accuracy by changing the things in feature extraction process like removing silence from the speech, considering more number of cepstral coefficients ( in this project I took only 12 cepstral coefficients ). How to deal with noisy speeches and speech with background music?
